{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization using the Gensim summarizer module\n",
    "\n",
    "This is a simple method to tackle the problem of text summarization, it is an extractive approach which select sentences from the corpus that best represent it and arrange them to form a summary. *Extractive summary extract the important and meaningful sentences from the original text and placing them into summary without any changes* [1].\n",
    "As described in an excellent paper about extractive summarization, cited in the references section: \n",
    "\n",
    "*Extractive techniques generally generate summaries through 3 phases or it essentially based on them. These phases are preprocessing step,\n",
    "processing step and generation step:\n",
    "1) Preprocessing step: the representation space dimensionality of the original text is reduced to involve a new structure representation. It usually includes:\n",
    "\n",
    "    a. Stop-word elimination: Common words without semantics that do not collect information relevant to the task (for example, \"the\", \"a\", \"an\", \"in\") are eliminated.\n",
    "    \n",
    "    b. Steaming: Acquire the stem of each word by bringing the word to its base form.\n",
    "    \n",
    "    c. Part of speech tagging: The process of identifying and classifying words of the text on the basis of part of speech category they belong (nouns, verbs, adverbs, adjectives).\n",
    "    \n",
    "2) Processing step: It uses an algorithm with the help of features generated in the preprocessing step to convert the text structure to the summary structure. In which, the sentences are scored.\n",
    "\n",
    "3) Generation step: sentences are ranked. Then, it pick up the most important sentences from the ranked structure to generate the final required summary.*\n",
    "\n",
    "These techniques are very popular in the industry as they are very easy to implement. They use existing natural language phrases and are reasonably accurate. And they are very fast since they are an unsupervised algorithm, so they do not have to calculate loss function in every step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the gensim package for the first time only\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the rouge library to calculate the ROUGE metrics to evaluate the results.This library is independant from the \"official\" ROUGE script (aka. ROUGE-155) and results may be slighlty different,but it is very easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.14.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#Load the gensim modules\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.summarization import summarizer\n",
    "#Module for splitting the dsataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import library to calculate the evaluation metric\n",
    "from rouge import FilesRouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when new datafiles have been stored in GS\n",
    "#%%bash\n",
    "#gsutil cp gs://mlend_bucket/data/news_summary/news_summary_more.csv ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.read_csv('../data/news_summary.csv', encoding='iso-8859-1')\n",
    "#raw = pd.read_csv('../data/news_summary_more.csv', encoding='iso-8859-1')\n",
    "summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                                text  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate rows\n",
    "summary.drop_duplicates(subset=[\"ctext\"],inplace=True)\n",
    "#Drop rows with null values in the text variable\n",
    "summary.dropna(inplace=True)\n",
    "summary.reset_index(drop=True,inplace=True)\n",
    "# we are using the text variable as the summary and the ctext as the source text\n",
    "dataset = summary[['headlines','ctext']].copy()\n",
    "dataset.columns = ['summary','text']\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess and cleanings\n",
    "\n",
    "Lets dive into the dataset to verify come cleanings we need to apply to our dataset.\n",
    "- Expand contractions\n",
    "- Puntuaction separation\n",
    "- Remove multispaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "punct = \"/-'.,?!#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def expand_contractions(text):\n",
    "    ''' Expand the contractions (some well-known of them) in a text'''\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    re_mult_space = re.compile(r\"  *\") # replace multiple spaces with just one\n",
    "    return re_mult_space.sub(r' ', text)\n",
    "\n",
    "def sep_punctuation(text, punct):\n",
    "# Separate punctuation with whitespaces\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f'{p} ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_CTL(text):\n",
    "    url = re.compile(r'\\n')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "def clean_text(text):\n",
    "    new_text=text\n",
    "    new_text=new_text.apply(lambda x : expand_contractions(x))\n",
    "    new_text=new_text.apply(lambda x : sep_punctuation(x,punct))\n",
    "    new_text=new_text.apply(lambda x : remove_mult_spaces(x))\n",
    "    new_text=new_text.apply(lambda x : remove_CTL(x))\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text=clean_text(dataset['text'])\n",
    "dataset_summary=clean_text(dataset['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into a train and a test set\n",
    "\n",
    "\n",
    "Although it is not strictly necessary, we will divide the data set into an input set and a test set in order \n",
    "to use the latter to evaluate the performance of the algorithm. The gensim summarizer only works on the source text we provide to produce the summary, it does not use any other data or information so it does matter if we split or how we split the data.\n",
    "\n",
    "Lets divide the data into a 20% for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  3472 3472\n",
      "Length test set:  869 869\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(dataset_text, dataset_summary, test_size=0.2, random_state=42, shuffle=True)\n",
    "print('Length train set: ',len(train_x),len(train_y))\n",
    "print('Length test set: ',len(test_x),len(test_y))\n",
    "train_x.reset_index(drop=True, inplace=True)\n",
    "train_y.reset_index(drop=True, inplace=True)\n",
    "test_x.reset_index(drop=True, inplace=True)\n",
    "test_y.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the summarizer with gensim\n",
    "\n",
    "**DESCRIBE HOW WE ARE GOING TO BUILD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the desire length for the summarize to produce\n",
    "summary_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data_text, summary_length):\n",
    "    ''' Generate a summary for every element in the data_text using the\n",
    "        gensim summarizer method.\n",
    "        \n",
    "        Input:\n",
    "           - data_text: list of strings to summarize\n",
    "           - summary_lenght: how many words will the summary contain\n",
    "        Output:\n",
    "           - A list of strings, the summary for every string in the data_text input\n",
    "           - errors: number of strings that can not be summarize\n",
    "           - no_summarizables: number of strings \n",
    "    '''\n",
    "    \n",
    "    summaries=[]\n",
    "    errors=0\n",
    "    no_summarizable=0\n",
    "    # Set the minimun of sentences in a text to be summarize\n",
    "    summarizer.INPUT_MIN_LENGTH = 3\n",
    "    # for every string in the input list\n",
    "    for i, source_text in enumerate(data_text):\n",
    "        # if the number of sentences in the source text is less thsn 1 \n",
    "        #it can not be summarize\n",
    "        if len(split_sentences(source_text))> 1:\n",
    "            try:\n",
    "                # Sometimes the sentences in the surce text are not correct\n",
    "                # to produce a summary\n",
    "                summaries.append(summarize(source_text, word_count = summary_lenght))\n",
    "            except:\n",
    "                summaries.append('Error')\n",
    "                errors +=1\n",
    "        else:\n",
    "            summaries.append('No Summarizable')\n",
    "            no_summarizable +=1\n",
    "\n",
    "    return summaries, errors, no_summarizable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating the summary for the train set\n",
      "\n",
      "Errors:  1\n",
      "No Summarizable:  33\n",
      "\n",
      "Generating the summary for the test set\n",
      "\n",
      "Errors:  0\n",
      "No Summarizable:  13\n"
     ]
    }
   ],
   "source": [
    "# Generate the summaries for the data and test set\n",
    "print(\"\\nGenerating the summary for the train set\\n\")\n",
    "train_preds,errors, no_summarizable=generate_summary(train_x, summary_length)\n",
    "print('Errors: ',errors)\n",
    "print('No Summarizable: ',no_summarizable)\n",
    "print(\"\\nGenerating the summary for the test set\\n\")\n",
    "test_preds,errors, no_summarizable=generate_summary(test_x, summary_length)\n",
    "print('Errors: ',errors)\n",
    "print('No Summarizable: ',no_summarizable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set there are any errors but 13 no summarizable texts. Lets explore some of them summaries we have extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: \n",
      "\n",
      "i:  25  :  While Deoband is known for its historical significance and Darul Uloom Deoband has been a centre of Islamic learning since the Mughal era, Brijesh Singh had demanded that the city be renamed Deovrind after BJP came into power. \n",
      "\n",
      "i:  23  :  Maharashtra' s bar dancers have waltzed up to the Supreme Court and argued that the prevailing ban- like atmosphere in the state could push them into activities like prostitution. \n",
      "\n",
      "i:  48  :   \n",
      "\n",
      "i:  28  :   \n",
      "\n",
      "i:  36  :  The MMRDA said it fixed 353 potholes on the Western Express Highway, which were reported between July 1 and July 10, last week. \n",
      "\n",
      "i:  49  :   \n",
      "\n",
      "i:  1  :   \n",
      "\n",
      "i:  42  :  ALSO READ: Ratan Tata does not speak the truth: Cyrus Mistry In the next few weeks, at least six Tata Group companies are expected to hold such meetings to remove Cyrus Mistry from the position of director. \n",
      "\n",
      "i:  12  :  New Delhi, Feb 19 ( PTI) FAQs about menstruation will now be answered at Delhi government schools with an NGO conducting \" period talks\" for teenage girls. \n",
      "\n",
      "i:  4  :  ( Dream- come- true, could not have asked for more: Kuldeep Yadav) He revealed that he has learnt a few tricks of the trade from the Aussie spin wizard Shane Warne and it was great to get his countryman with one of those. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#Print some summaries to analyze them\n",
    "print('Examples: \\n')\n",
    "for i in random.sample(range(50),10):\n",
    "    print('i: ',i,' : ', test_preds[i],'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have inspected the results we observed that there are three kind of errors or incoherent results: \n",
    "- Errors, when the gensim summarizer produce an error\n",
    "- No summarizable, when the summarizer can not applyied because of the number of sentences\n",
    "- Null, when the summary obtained is empty or null\n",
    "\n",
    "So we need to discard this rows from our predicted summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors:  0\n",
      "No summarizable:  13\n",
      "Nulls summaries:  66\n"
     ]
    }
   ],
   "source": [
    "#Search for the Error rows\n",
    "test_errors = np.where(np.array(test_preds)=='Error')[0]\n",
    "print('Errors: ',len(test_errors))\n",
    "##Search for the No summarizable text in the test set\n",
    "test_no_summa = np.where(np.array(test_preds)=='No Summarizable')[0]\n",
    "print('No summarizable: ',len(test_no_summa))\n",
    "##Search for the nulls sumaries in the test set\n",
    "test_nulls = np.where(np.array(test_preds)=='')[0]\n",
    "print('Nulls summaries: ',len(test_nulls))\n",
    "\n",
    "#Discard the incorrect summaries\n",
    "incorrect_preds=np.concatenate((test_errors,test_no_summa,test_nulls))\n",
    "predicted_summaries = [test_preds[i] for i in range(len(test_preds)) if i not in incorrect_preds]\n",
    "labeled_summaries =  [test_y[i] for i in range(len(test_y)) if i not in incorrect_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790, 790, 790)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_summaries),len(predicted_summaries),len(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the results using ROUGE metrics \n",
    "\n",
    "**DESCRIBE ROUGE METRIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predicted and real summaries file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_textfile(filename, strings):\n",
    "    ''' Save the contect of a list of strings to a file called filename\n",
    "    \n",
    "        Input:\n",
    "           - filename: name of the file to save the strings\n",
    "           - strings: a list of string to save to disk\n",
    "    '''\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        for item in strings:\n",
    "            #Remove any \\n in the string\n",
    "            item = remove_CTL(item)\n",
    "            f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the files with the predicted summaries\n",
    "save_textfile('predicted_summaries.txt',predicted_summaries)\n",
    "save_textfile('labeled_summaries.txt',labeled_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the files with the predicted and real summaries are stored, withone summary per line, we can call the rouge library to get the metrics for our evaluation method.\n",
    "Lets try it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the rouge object and scores the ROUGE metrics in average \n",
    "files_rouge = FilesRouge()\n",
    "scores = files_rouge.get_scores('predicted_summaries.txt', 'labeled_summaries.txt', avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.15058549183980122, 'p': 0.10255481089937296, 'r': 0.306434167098725}, 'rouge-2': {'f': 0.03799941783852152, 'p': 0.02541178949636445, 'r': 0.08221898003543593}, 'rouge-l': {'f': 0.13370336655643658, 'p': 0.09253319907707971, 'r': 0.2580442272214432}}\n"
     ]
    }
   ],
   "source": [
    "#print('Average Results on the Test set:\\n')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two aspects that may impact the need for human post-processing:\n",
    "- Does the summary sound fluent?\n",
    "- Is summary adequate? I.e. is the length appropriate and does it cover the most important information of the text it summarizes?\n",
    "\n",
    "ROUGE doesn't try to assess how fluent the summary: ROUGE only tries to assess the adequacy, by simply counting how many n-grams in your generated summary matches the n-grams in your reference summary (or summaries, as ROUGE supports multi-reference corpora).This is the process for one document-summary pair. You repeat the process for all documents, and average all the scores and that gives you a ROUGE-N score. So a higher score would mean that on average there is a high overlap of n-grams between your summaries and the references.\n",
    "\n",
    "Example:\n",
    "\n",
    "S1. police killed the gunman\n",
    "S2. police kill the gunman\n",
    "S3. the gunman kill police\n",
    "\n",
    "S1 is the reference and S2 and S3 are candidates. Note S2 and S3 both have one overlapping bigram with the reference, so they have the same ROUGE-2 score, although S2 should be better. An additional ROUGE-L score deals with this, where L stands for Longest Common Subsequence. In S2, the first word and last two words match the reference, so it scores 3/4, whereas S3 only matches the bigram, so scores 2/4. See the paper for more details\n",
    "\n",
    "Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree.\n",
    "\n",
    "**What is the best way to really understand what a ROUGE score actually measures?**\n",
    "\n",
    "In short and approximately:\n",
    "\n",
    "ROUGE-n recall=40% means that 40% of the n-grams in the reference summary are also present in the generated summary.\n",
    "ROUGE-n precision=40% means that 40% of the n-grams in the generated summary are also present in the reference summary.\n",
    "ROUGE-n F1-score=40% is more difficult to interpret, like any F1-score.\n",
    "ROUGE is more interpretable than BLEU (from {2}: \"Other Known Deficiencies of Bleu: Scores hard to interpret\"). I said approximately because the original ROUGE implementation from the paper that introduced ROUGE {3} may perform a few more things such as stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to check the count of lines in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines is: 790\n"
     ]
    }
   ],
   "source": [
    "fname = \"predicted_summaries.txt\"\n",
    "count = 0\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "print(\"Total number of lines is:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1]. El-Refaiy, Ahmed & Abas, A.R. & Elhenawy, I.. (2018). Review of recent techniques for extractive text summarization. Journal of Theoretical and Applied Information Technology. 96. 7739-7759. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] Lin, Chin-Yew. \"Rouge: A package for automatic evaluation of summaries.\" In Text summarization branches out: Proceedings of the ACL-04 workshop, vol. 8. 2004. https://scholar.google.com/scholar?cluster=2397172516759442154&hl=en&as_sdt=0,5 ; http://anthology.aclweb.org/W/W04/W04-1013.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
