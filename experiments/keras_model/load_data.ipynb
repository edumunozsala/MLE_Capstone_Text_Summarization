{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(bucket_name, source, destination):\n",
    "    #search = re.search('gs://(.*?)/(.*)', source)\n",
    "    #bucket_name = search.group(1)\n",
    "    #blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(source).download_to_filename(destination)\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_news_data(bucket, train_data_path, train_filename, eval_data_path, eval_filename):\n",
    "#    if train_data_path.startswith('gs://'):\n",
    "    download_from_gcs(bucket, train_data_path, destination=train_filename)\n",
    "    train_data_path = train_filename\n",
    "#    if eval_data_path.startswith('gs://'):\n",
    "    download_from_gcs(bucket,eval_data_path, destination=eval_filename)\n",
    "    eval_data_path = eval_filename\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep=',',header=0)\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep=',',header=0)\n",
    "\n",
    "    return ((list(df_train['text']), list(df_train['label'])),\n",
    "            (list(df_eval['text']), list(df_eval['label'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs_data_path='gs://mlend_text_summarization/data/amazon-fine-food-reviews'\n",
    "bucket='mlend_text_summarization'\n",
    "gs_data_path='data/amazon-fine-food-reviews/'\n",
    "train_filename='train.csv'\n",
    "eval_filename='validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from GS to local disk\n",
    "    ((train_texts, train_labels), (test_texts, test_labels)) = load_news_data(\n",
    "        bucket,gs_data_path+train_filename,train_filename,\n",
    "        gs_data_path+eval_filename,eval_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  776  target:  776\n",
      "Eval data:  138  target:  138\n"
     ]
    }
   ],
   "source": [
    "print('Train data: ',len(train_texts),' target: ',len(train_labels))\n",
    "print('Eval data: ',len(test_texts),' target: ',len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data to be consumed by the model\n",
    "\n",
    "Remember to add the START and END special tokens at the beginning and end of the summary. Here, I have chosen sostok and eostok as START and END tokens\n",
    "\n",
    "Note: Be sure that the chosen special tokens never appear in the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token='eostok'\n",
    "sos_token='sostok'\n",
    "\n",
    "def include_EOS_SOS(texts, eos_token, sos_token):\n",
    "    return list(map(lambda x: sos_token+' '+x+' '+eos_token, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token='eostok'\n",
    "sos_token='sostok'\n",
    "#result = list(map(lambda x: sos_token+' '+x+' '+eos_token, train_labels))\n",
    "result = include_EOS_SOS(train_labels, eos_token, sos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Tokenizer\n",
    "\n",
    "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Go ahead and build tokenizers for text and summary:\n",
    "\n",
    "Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire text\n",
    "\n",
    "Here, I am defining the threshold to be 4 which means word whose count is below 4 is considered as a rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 76.23762376237624\n",
      "Total Coverage of rare words: 23.87706855791962\n"
     ]
    }
   ],
   "source": [
    "def set_vocab_threshold(thresh,tokenizer)\n",
    "\n",
    "    cnt=0\n",
    "    tot_cnt=0\n",
    "    freq=0\n",
    "    tot_freq=0\n",
    "\n",
    "    for key,value in tokenizer.word_counts.items():\n",
    "        tot_cnt=tot_cnt+1\n",
    "        tot_freq=tot_freq+value\n",
    "        if(value<thresh):\n",
    "            cnt=cnt+1\n",
    "            freq=freq+value\n",
    "    return cnt, tot_cnt\n",
    "\n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
