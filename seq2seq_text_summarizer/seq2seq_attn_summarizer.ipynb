{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization using a seq2seq with attention model on Pytorch\n",
    "\n",
    "**DESCRIBE WHAT IS THE ALGO.**\n",
    "\n",
    "Link:\n",
    "https://www.kaggle.com/rahuldshetty/text-summarization-in-pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.switch_backend('agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Import library to calculate the evaluation metric\n",
    "#from rouge import FilesRouge, Rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../data'\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "glove_filename = os.path.join(data_path, glove_file)\n",
    "#Define some especial tokens for our vocabulary\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when new datafiles have been stored in GS\n",
    "#%%bash\n",
    "#gsutil cp gs://mlend_bucket/data/news_summary/news_summary_more.csv ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.read_csv('MLE_Capstone_Text_Summarization/data/news_summary.csv', encoding='iso-8859-1')\n",
    "summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                                text  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate rows\n",
    "summary.drop_duplicates(subset=[\"ctext\"],inplace=True)\n",
    "#Drop rows with null values in the text variable\n",
    "summary.dropna(inplace=True)\n",
    "summary.reset_index(drop=True,inplace=True)\n",
    "# we are using the text variable as the summary and the ctext as the source text\n",
    "dataset = summary[['text','ctext']].copy()\n",
    "dataset.columns = ['summary','text']\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess and cleanings\n",
    "\n",
    "Lets dive into the dataset to verify come cleanings we need to apply to our dataset.\n",
    "- Expand contractions\n",
    "- Puntuaction separation\n",
    "- Remove multispaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "punct = \"/-'.,?!#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "def expand_contractions(text):\n",
    "    ''' Expand the contractions (some well-known of them) in a text'''\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    re_mult_space = re.compile(r\"  *\") # replace multiple spaces with just one\n",
    "    return re_mult_space.sub(r' ', text)\n",
    "\n",
    "def sep_punctuation(text, punct):\n",
    "# Separate punctuation with whitespaces\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f'{p} ')\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_CTL(text):\n",
    "    url = re.compile(r'\\n')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "def clean_text(text):\n",
    "    new_text=text\n",
    "    new_text=new_text.apply(lambda x : expand_contractions(x))\n",
    "    new_text=new_text.apply(lambda x : sep_punctuation(x,punct))\n",
    "    new_text=new_text.apply(lambda x : remove_mult_spaces(x))\n",
    "    new_text=new_text.apply(lambda x : remove_CTL(x))\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text']=clean_text(dataset['text'])\n",
    "dataset['summary']=clean_text(dataset['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split every document into a list of sentences\n",
    "dataset['list_sentences'] = split_sentences(dataset['text'].values)\n",
    "dataset['list_summary'] = split_sentences(dataset['summary'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data in training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_target, val_target = train_test_split(dataset['list_sentences'].values, dataset['list_summary'].values, \n",
    "                                                                  test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train Text:  3689  Length Train Target:  3689\n",
      "Length Val Text:  652  Length Train Target:  652\n"
     ]
    }
   ],
   "source": [
    "print('Length Train Text: ',len(train_data), ' Length Train Target: ', len(train_target))\n",
    "print('Length Val Text: ',len(val_data), ' Length Train Target: ', len(val_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the vocabulary for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize or split a text in sentences\n",
    "def split_sentences(documents):\n",
    "    \"\"\"\n",
    "    Splits the documents into individual sentences.\n",
    "    Input:\n",
    "       - documents: list of string to split into sentences\n",
    "    Output:\n",
    "        - list of list of strings (sentence)\n",
    "    \"\"\"\n",
    "    docs_sentences=[]\n",
    "    n_docs = len(documents)\n",
    "    for i in range(n_docs):\n",
    "        text = documents[i]\n",
    "        #print(email)\n",
    "        sentences = sent_tokenize(text)\n",
    "        #print(sentences)\n",
    "        for j in reversed(range(len(sentences))):\n",
    "            sent = sentences[j]\n",
    "            sentences[j] = sent.strip()\n",
    "            if sent == '':\n",
    "                sentences.pop(j)\n",
    "        docs_sentences.append(sentences)\n",
    "        \n",
    "    return docs_sentences\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def fit(self, data):\n",
    "        ''' data: list of list of string sentence\n",
    "        '''\n",
    "        #tokens = [sentence.lower().split() for doc in data for sentence in doc]\n",
    "        tokens= [word_tokenize(sentence.lower()) for doc in data for sentence in doc]\n",
    "        tokens= [token for sentence in tokens for token in sentence]\n",
    "\n",
    "        for word in tokens:\n",
    "            self.addWord(word)\n",
    "        \n",
    "        self.addWord('<UNK>')\n",
    "        \n",
    "    def transform(self, data, eos_token=True):\n",
    "    # determine the dimensionality of vectors\n",
    "    \n",
    "        errors=0\n",
    "        docs2int=[]\n",
    "        for doc in data:\n",
    "            doc2int=[]\n",
    "            for sentence in doc:\n",
    "                tokens = word_tokenize(sentence.lower())\n",
    "                #vecs=[]\n",
    "                for word in tokens:\n",
    "                    try:\n",
    "                      # throws KeyError if word not found\n",
    "                        vec = self.word2index[word]\n",
    "                        doc2int.append(vec)\n",
    "                    except KeyError:\n",
    "                        errors +=1\n",
    "      \n",
    "                #if len(vecs) > 0:\n",
    "                #    sent2int.append(vecs)\n",
    "            if len(doc2int) > 0:\n",
    "                if eos_token:\n",
    "                    doc2int.append(EOS_token)\n",
    "                    \n",
    "                docs2int.append(doc2int)\n",
    "    \n",
    "        return docs2int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the vocabulary for the documents data and the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Glove embeddings\n",
    "#data= dataset['list_sentences'].values\n",
    "#data_sum= dataset['list_summary'].values\n",
    "#data= dataset['list_sentences'].values\n",
    "#data_sum= dataset['list_summary'].values\n",
    "\n",
    "#print(data)\n",
    "vocab_text = Vocabulary('text')\n",
    "vocab_text.fit(train_data)\n",
    "#print(len(vocab_text.word2index))\n",
    "#vocab_text.fit(data_sum)\n",
    "#print(len(vocab_text.word2index))\n",
    "\n",
    "vocab_summary = Vocabulary('summary')\n",
    "vocab_summary.fit(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20945, 55983)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_summary.word2index),len(vocab_text.word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the vocabulary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 367)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[word_tokenize(sentence.lower()) for doc in data_sum for sentence in doc]\n",
    "sum=0\n",
    "for tok in tokens:\n",
    "    sum+=len(tok)\n",
    "    \n",
    "len(vocab_summary.word2index),sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum(vocab_text.word2count.values())\n",
    "d=list(vocab_summary.word2count.values())\n",
    "#print(d)\n",
    "da=np.array(d)\n",
    "np.sum(da)\n",
    "#d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the input and output indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Glove embeddings\n",
    "#data = dataset['list_sentences'].values\n",
    "#data_sum= dataset['list_summary'].values\n",
    "#data_sum= dataset['list_summary'].values[:1]\n",
    "\n",
    "inputs=vocab_text.transform(train_data)\n",
    "outputs=vocab_summary.transform(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14352"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(ip) for ip in inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the text transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20945, 1762613)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[word_tokenize(sentence.lower()) for doc in data for sentence in doc]\n",
    "sum=0\n",
    "for tok in tokens:\n",
    "    sum+=len(tok)\n",
    "    \n",
    "len(vocab_summary.word2index),sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20945, 1762613)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inp=[tk for doc in data for tk in doc]\n",
    "sum=0\n",
    "for tok in inputs:\n",
    "    sum+=len(tok)\n",
    "    \n",
    "len(vocab_summary.word2index),sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pytorch Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for an enconder instance\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a decoder instance\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a decoder with attention mechanism class\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainStep(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          teacher_forcing_ratio, criterion, max_length=100):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    #print('Input Length: ',input_length)\n",
    "    target_length = target_tensor.size(0)\n",
    "    #print('Target Length: ',target_length)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "#            print('di:', str(di))\n",
    "#            print('TargTensor: ',target_tensor.shape)\n",
    "#            print('DecOutput: ',decoder_output.shape)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "#            print('di:', str(di))\n",
    "#            print('TargTensor: ',target_tensor.shape)\n",
    "#            print('DecOutput: ',decoder_output.shape)\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(inputs, targets, encoder, decoder, n_iters, print_every=1000, plot_every=100, \n",
    "               learning_rate=0.01, teacher_forcing_ratio=0.5, max_length=100):\n",
    "    '''\n",
    "    inputs: a list of tensors from the list of integer_token\n",
    "    targets: a list of tensors from the list of integer_token\n",
    "    '''\n",
    "    print(\"Training....\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                  for i in range(n_iters)]\n",
    "    input_training = random.sample(inputs, n_iters)\n",
    "    target_training = random.sample(targets, n_iters)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        # If the length of the text in tokens is greater the max_length, skip it\n",
    "        if len(input_training[iter-1]) > max_length:\n",
    "            continue\n",
    "        #training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = torch.tensor(input_training[iter-1], dtype=torch.long, device=device).view(-1, 1)\n",
    "        #print('Input Shape: ',input_tensor.shape)\n",
    "        \n",
    "        #input_training[iter - 1]\n",
    "        target_tensor = torch.tensor(target_training[iter-1], dtype=torch.long, device=device).view(-1, 1)\n",
    "        #print('Output Shape: ',target_tensor.shape)\n",
    "\n",
    "        #targets[iter - 1]\n",
    "\n",
    "        loss = trainStep(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "                         teacher_forcing_ratio, criterion, max_length)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "2m 13s (- 20m 2s) (100 10%) 8.0722\n",
      "4m 30s (- 18m 2s) (200 20%) 7.3767\n",
      "6m 32s (- 15m 16s) (300 30%) 6.8954\n",
      "8m 37s (- 12m 55s) (400 40%) 6.3201\n",
      "10m 34s (- 10m 34s) (500 50%) 5.8797\n",
      "12m 37s (- 8m 25s) (600 60%) 5.7803\n",
      "14m 59s (- 6m 25s) (700 70%) 5.7883\n",
      "17m 9s (- 4m 17s) (800 80%) 5.6219\n",
      "19m 11s (- 2m 7s) (900 90%) 5.8143\n",
      "1000 / 1001\n",
      "21m 26s (- 0m 0s) (1000 100%) 6.1243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "encoder1 = EncoderRNN(vocab_text.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, vocab_summary.n_words, dropout_p=0.1, max_length=5000).to(device)\n",
    "\n",
    "trainIters(inputs, outputs, encoder1, attn_decoder1, 1000,  print_every=100, plot_every=50, max_length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained,it will be saved to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), './enc.w')\n",
    "torch.save(attn_decoder1.state_dict(), './att.w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_summary(encoder, decoder, input, vocab_output, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(input, dtype=torch.long, device=device).view(-1, 1)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab_output.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words      #, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'the', 'the', 'has', 'has', 'and', 'the', \"'\", 's', 'the', 'of', 'the', 'the', 'the', '.', 'the', 'the', 'the', 'the', '.', 'the', 'the', '.', 'the', 'the', 'the', '.', 'the', 'the', '.', 'the', 'the', '.', 'the', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', 'the', '.', 'the', '.', 'the', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', '.', 'the', '.', 'the', '.', 'the', '.', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', 'the', '.', '.', '<EOS>']\n",
      "tensor([[2.2370e-04, 1.8803e-04, 6.0297e-04,  ..., 3.4757e-04, 8.7536e-05,\n",
      "         9.8834e-05],\n",
      "        [8.8325e-05, 1.8362e-04, 1.3592e-04,  ..., 2.8329e-04, 1.2433e-04,\n",
      "         6.1018e-05],\n",
      "        [1.1514e-04, 2.2382e-04, 1.3763e-04,  ..., 3.4903e-04, 2.4179e-04,\n",
      "         1.0690e-04],\n",
      "        ...,\n",
      "        [1.0485e-04, 8.5703e-05, 1.4323e-04,  ..., 2.7821e-04, 1.7955e-04,\n",
      "         5.6625e-05],\n",
      "        [1.2300e-04, 1.8105e-04, 9.7180e-05,  ..., 3.5546e-04, 2.5213e-04,\n",
      "         1.2330e-04],\n",
      "        [1.6213e-04, 1.4749e-04, 1.5405e-04,  ..., 4.3607e-04, 1.7338e-04,\n",
      "         7.5793e-05]])\n"
     ]
    }
   ],
   "source": [
    "#predict_summary(encoder, decoder, input, max_length=100)\n",
    "dec_sent = predict_summary(encoder1, attn_decoder1, inputs[0], vocab_summary, max_length=5000)\n",
    "print(dec_sent)\n",
    "print(dec_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate the predicted summaries for our vaildation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranforme the text to the indexes in the vocabulary\n",
    "inputs=vocab_text.transform(val_data)\n",
    "outputs=vocab_summary.transform(val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the the the has has and the ' s ' s the of the the . the the the . the the the . the the the . the the . the . the . the the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . the . . the . . the . . the . the . the . the . the . the . . the . . the . the . the . the . the . the . the . . the . . the . the . . the . the . the . . the . the . the . the . the . . <EOS>\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the summary for the vaildation dataset\n",
    "#predicted_summaries = \n",
    "predicted_summaries = [' '.join(predict_summary(encoder1, attn_decoder1, inputs[0], vocab_summary, max_length=5000)) for input in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
