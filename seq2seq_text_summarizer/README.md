
# Capstone Project about Text Summarization using Machine Learning techniques


This folder of the repository contains the notebook where we implement the abstractive text summarization model based on Encoder-Decoder with attention:
- Sequence-2-sequence-with-attention-summarizer

## Requirements

To run the notebook you need the most common libraries in python like pandas, numpy, matplotlib, os, io, random, pickle, re, string, nltk, sklearn,...

But you also need PYTORCH version 1.4.0. It is easily installed using pip install: pip install torch

Other package you need to download and install using the pip command is the rouge package. At the begining of the notebook you can install that packag just running a cell with the commands:

!pip install rouge
(You should uncomment this lines)

## How to run it

**We highly do not recommend to test this model** it was discarded as a good solution and takes many hours to run.

This notebook has been trainned on Kaggle notebooks because it provide GPU resources for free and easy tu use. There are some commented lines that the kaggle enviroment insert automatically. 

In Kaggle you can access to the code in this [link](https://www.kaggle.com/edumunozsala/sequence-2-sequence-with-attention-summarize)

The datasets need it to run te model are uploaded to Kaggle and added to the notebook. The datasets are:
- Cleaned News summary
- GloVe: Global Vectors for Word Representation

In the third code cell we set the value for some global variables that , **you must review and modify depending on where the datafiles are stored** and what is the current directory. Those variables are:

data_path = '/kaggle/input/news-summary/news_summary_more.csv'
valid_path = '/kaggle/input/news-summary/news_summary_more.csv'

The output files will be stored in the default output path provided by the kaggle enviroment.

In the setion **Training the model** we define the two main parametersin this model:
- hidden size = 300
- iterations = 225,000

You can modify these values to "play around" or test the model.

But **This model takes about 6-8 hours** to train on 98,000 examples and predict on the same whole dataset. 

## Data

The files containing the data are included in the data directory of this repository.

We said previously that you can find the data on Kaggle, Cleaned News summary and Glove vector. 

We have cleaned and processed the data previously using the techniques included in the notebook in the folder data_analysis.

You can download our cleaned dataset in a Kaggle public dataset called [Cleaned News Summary](https://www.kaggle.com/edumunozsala/cleaned-news-summary).
You can also download the Glove embeddings from Kaggle in the folowing dataset [GloVe: Global Vectors for Word Representation](https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation), glove.6B.100d.txt.

## Sequence-2-Sequence model

The encoder-decoder model for recurrent neural networks is an architecture for sequence-to-sequence prediction problems. It comprised two parts:
-	Encoder: The encoder is responsible for stepping through the input time steps, read the input words one by one and encoding the entire sequence into a fixed length vector called a context vector.
-	Decoder: The decoder is responsible for stepping through the output time steps while reading from the context vector, extracting the words one by one.
The trouble with seq2seq is that the only information that the decoder receives from the encoder is the last encoder hidden state which is like a numerical summary of an input sequence. So, for a long input text, we expect the decoder to use just this one vector representation to output a translation. This might lead to catastrophic forgetting.

To solve this problem, the attention mechanism was developed. Attention is proposed as a method to both align and translate. It identifies which parts of the input sequence are relevant to each word in the output (alignment) and use that relevant information to select the right output (translation). So instead of encoding the input sequence into a single fixed context vector (reason for the mentioned bad performance), the attention model develops a context vector that is filtered specifically for each output time step. Attention provides the decoder with information from every encoder hidden state. With this setting, the model can selectively focus on useful parts of the input sequence and hence, learn the alignment between them.

We will apply Teacher forcing, a method for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input, that is, using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network"*


## License
This repository is under the GNU General Public License v3.0.
